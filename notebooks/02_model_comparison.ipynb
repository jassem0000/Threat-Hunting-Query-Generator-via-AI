{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Model Comparison for Threat Hunting Query Generation\n",
                "\n",
                "This notebook compares different LLM models (Llama 3.2, Mistral 7B, Gemma 2) for query generation.\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "Make sure you have the following models installed via Ollama:\n",
                "```bash\n",
                "ollama pull llama3.2\n",
                "ollama pull mistral\n",
                "ollama pull gemma2:9b\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "import time\n",
                "import pandas as pd\n",
                "import plotly.graph_objects as go\n",
                "import plotly.express as px\n",
                "from datetime import datetime\n",
                "\n",
                "# Configuration\n",
                "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
                "MODELS = [\n",
                "    'llama3.2',\n",
                "    'mistral',\n",
                "    'gemma2:9b'\n",
                "]\n",
                "\n",
                "# Test scenarios\n",
                "TEST_SCENARIOS = [\n",
                "    {\n",
                "        'name': 'Brute Force Attack',\n",
                "        'description': 'Find failed login attempts indicating brute force attack',\n",
                "        'query_type': 'spl'\n",
                "    },\n",
                "    {\n",
                "        'name': 'Ransomware Detection',\n",
                "        'description': 'Detect ransomware by finding mass file encryption events',\n",
                "        'query_type': 'kql'\n",
                "    },\n",
                "    {\n",
                "        'name': 'Data Exfiltration',\n",
                "        'description': 'Identify large outbound data transfers to external destinations',\n",
                "        'query_type': 'dsl'\n",
                "    },\n",
                "    {\n",
                "        'name': 'PowerShell Obfuscation',\n",
                "        'description': 'Find obfuscated PowerShell commands with base64 encoding',\n",
                "        'query_type': 'spl'\n",
                "    },\n",
                "    {\n",
                "        'name': 'Lateral Movement',\n",
                "        'description': 'Detect lateral movement using RDP between hosts',\n",
                "        'query_type': 'kql'\n",
                "    }\n",
                "]\n",
                "\n",
                "def generate_query_with_model(model_name, description, query_type):\n",
                "    \"\"\"Generate query using specific model\"\"\"\n",
                "    \n",
                "    prompt = f\"\"\"Generate a {query_type.upper()} threat hunting query for the following:\n",
                "{description}\n",
                "\n",
                "Return ONLY the query, no explanations.\"\"\"\n",
                "    \n",
                "    start_time = time.time()\n",
                "    \n",
                "    try:\n",
                "        response = requests.post(\n",
                "            OLLAMA_URL,\n",
                "            json={\n",
                "                'model': model_name,\n",
                "                'prompt': prompt,\n",
                "                'stream': False\n",
                "            },\n",
                "            timeout=60\n",
                "        )\n",
                "        \n",
                "        generation_time = time.time() - start_time\n",
                "        \n",
                "        if response.status_code == 200:\n",
                "            result = response.json()\n",
                "            query = result.get('response', '')\n",
                "            \n",
                "            return {\n",
                "                'success': True,\n",
                "                'query': query,\n",
                "                'generation_time': generation_time,\n",
                "                'error': None\n",
                "            }\n",
                "        else:\n",
                "            return {\n",
                "                'success': False,\n",
                "                'query': None,\n",
                "                'generation_time': generation_time,\n",
                "                'error': f\"HTTP {response.status_code}\"\n",
                "            }\n",
                "    except Exception as e:\n",
                "        return {\n",
                "            'success': False,\n",
                "            'query': None,\n",
                "            'generation_time': time.time() - start_time,\n",
                "            'error': str(e)\n",
                "        }\n",
                "\n",
                "def evaluate_query_quality(query, query_type):\n",
                "    \"\"\"Basic quality evaluation of generated query\"\"\"\n",
                "    if not query:\n",
                "        return {'score': 0, 'has_syntax': False, 'has_fields': False, 'has_timerange': False}\n",
                "    \n",
                "    score = 0\n",
                "    query_lower = query.lower()\n",
                "    \n",
                "    # Check for basic syntax\n",
                "    if query_type == 'spl':\n",
                "        has_syntax = 'search' in query_lower or 'index=' in query_lower\n",
                "        has_fields = '|' in query or 'stats' in query_lower\n",
                "        has_timerange = 'earliest' in query_lower or 'latest' in query_lower\n",
                "    elif query_type == 'kql':\n",
                "        has_syntax = any(table in query for table in ['SecurityEvent', 'SigninLogs', 'DeviceEvents'])\n",
                "        has_fields = '|' in query or 'where' in query_lower\n",
                "        has_timerange = 'ago' in query_lower or 'between' in query_lower\n",
                "    else:  # dsl\n",
                "        has_syntax = 'query' in query_lower or 'bool' in query_lower\n",
                "        has_fields = 'must' in query_lower or 'filter' in query_lower\n",
                "        has_timerange = 'range' in query_lower\n",
                "    \n",
                "    if has_syntax:\n",
                "        score += 3\n",
                "    if has_fields:\n",
                "        score += 2\n",
                "    if has_timerange:\n",
                "        score += 2\n",
                "    \n",
                "    # Query length (not too short, not too long)\n",
                "    length = len(query)\n",
                "    if 50 < length < 500:\n",
                "        score += 2\n",
                "    elif length >= 500:\n",
                "        score += 1\n",
                "    \n",
                "    return {\n",
                "        'score': min(score, 10),  # Max score of 10\n",
                "        'has_syntax': has_syntax,\n",
                "        'has_fields': has_fields,\n",
                "        'has_timerange': has_timerange,\n",
                "        'length': length\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Setup complete. Ready to compare models!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store results\n",
                "comparison_results = []\n",
                "\n",
                "print(\"üî¨ Starting model comparison...\\n\")\n",
                "print(f\"Models to test: {', '.join(MODELS)}\")\n",
                "print(f\"Scenarios: {len(TEST_SCENARIOS)}\\n\")\n",
                "\n",
                "for scenario in TEST_SCENARIOS:\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"üìù Scenario: {scenario['name']}\")\n",
                "    print(f\"Query Type: {scenario['query_type'].upper()}\")\n",
                "    print(f\"{'='*80}\\n\")\n",
                "    \n",
                "    for model in MODELS:\n",
                "        print(f\"Testing {model}...\", end=' ')\n",
                "        \n",
                "        result = generate_query_with_model(\n",
                "            model,\n",
                "            scenario['description'],\n",
                "            scenario['query_type']\n",
                "        )\n",
                "        \n",
                "        if result['success']:\n",
                "            quality = evaluate_query_quality(result['query'], scenario['query_type'])\n",
                "            print(f\"‚úÖ ({result['generation_time']:.2f}s, Quality: {quality['score']}/10)\")\n",
                "        else:\n",
                "            quality = {'score': 0, 'has_syntax': False, 'has_fields': False, 'has_timerange': False, 'length': 0}\n",
                "            print(f\"‚ùå Error: {result['error']}\")\n",
                "        \n",
                "        comparison_results.append({\n",
                "            'scenario': scenario['name'],\n",
                "            'query_type': scenario['query_type'],\n",
                "            'model': model,\n",
                "            'success': result['success'],\n",
                "            'generation_time': result['generation_time'],\n",
                "            'quality_score': quality['score'],\n",
                "            'has_syntax': quality['has_syntax'],\n",
                "            'has_fields': quality['has_fields'],\n",
                "            'has_timerange': quality['has_timerange'],\n",
                "            'query_length': quality.get('length', 0),\n",
                "            'query': result['query']\n",
                "        })\n",
                "\n",
                "print(\"\\n\\n‚úÖ Comparison complete!\")\n",
                "\n",
                "# Convert to DataFrame\n",
                "df_results = pd.DataFrame(comparison_results)\n",
                "df_results.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Results Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics by model\n",
                "summary = df_results.groupby('model').agg({\n",
                "    'success': 'mean',\n",
                "    'generation_time': 'mean',\n",
                "    'quality_score': 'mean',\n",
                "    'has_syntax': 'mean',\n",
                "    'has_fields': 'mean',\n",
                "    'has_timerange': 'mean',\n",
                "    'query_length': 'mean'\n",
                "}).round(2)\n",
                "\n",
                "summary.columns = [\n",
                "    'Success Rate',\n",
                "    'Avg Generation Time (s)',\n",
                "    'Avg Quality Score',\n",
                "    'Syntax Correctness',\n",
                "    'Field Usage',\n",
                "    'Time Range',\n",
                "    'Avg Query Length'\n",
                "]\n",
                "\n",
                "print(\"\\nüìä MODEL COMPARISON SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(summary)\n",
                "print(\"\\n\")\n",
                "\n",
                "# Find best model\n",
                "best_quality = summary['Avg Quality Score'].idxmax()\n",
                "best_speed = summary['Avg Generation Time (s)'].idxmin()\n",
                "\n",
                "print(f\"üèÜ Best Quality: {best_quality} (Score: {summary.loc[best_quality, 'Avg Quality Score']}/10)\")\n",
                "print(f\"‚ö° Fastest: {best_speed} ({summary.loc[best_speed, 'Avg Generation Time (s)']}s)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization 1: Generation Time Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generation time comparison\n",
                "fig = px.box(\n",
                "    df_results,\n",
                "    x='model',\n",
                "    y='generation_time',\n",
                "    color='model',\n",
                "    title='Query Generation Time by Model',\n",
                "    labels={'generation_time': 'Generation Time (seconds)', 'model': 'LLM Model'}\n",
                ")\n",
                "fig.update_layout(showlegend=False)\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization 2: Quality Score Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quality score comparison\n",
                "fig = go.Figure()\n",
                "\n",
                "for model in MODELS:\n",
                "    model_data = df_results[df_results['model'] == model]\n",
                "    fig.add_trace(go.Bar(\n",
                "        name=model,\n",
                "        x=model_data['scenario'],\n",
                "        y=model_data['quality_score'],\n",
                "    ))\n",
                "\n",
                "fig.update_layout(\n",
                "    title='Quality Score by Scenario and Model',\n",
                "    xaxis_title='Scenario',\n",
                "    yaxis_title='Quality Score (0-10)',\n",
                "    barmode='group'\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization 3: Feature Comparison Radar Chart"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Radar chart for model capabilities\n",
                "categories = ['Success Rate', 'Quality Score', 'Syntax Correctness', 'Field Usage', 'Time Range']\n",
                "\n",
                "fig = go.Figure()\n",
                "\n",
                "for model in MODELS:\n",
                "    model_summary = summary.loc[model]\n",
                "    values = [\n",
                "        model_summary['Success Rate'] * 10,  # Scale to 10\n",
                "        model_summary['Avg Quality Score'],\n",
                "        model_summary['Syntax Correctness'] * 10,\n",
                "        model_summary['Field Usage'] * 10,\n",
                "        model_summary['Time Range'] * 10\n",
                "    ]\n",
                "    \n",
                "    fig.add_trace(go.Scatterpolar(\n",
                "        r=values,\n",
                "        theta=categories,\n",
                "        fill='toself',\n",
                "        name=model\n",
                "    ))\n",
                "\n",
                "fig.update_layout(\n",
                "    polar=dict(\n",
                "        radialaxis=dict(\n",
                "            visible=True,\n",
                "            range=[0, 10]\n",
                "        )),\n",
                "    title='Model Capabilities Comparison'\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detailed Query Comparison (Example)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pick a scenario and show all model outputs\n",
                "scenario_name = TEST_SCENARIOS[0]['name']\n",
                "scenario_results = df_results[df_results['scenario'] == scenario_name]\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"üìù Detailed Comparison for: {scenario_name}\")\n",
                "print(f\"{'='*80}\\n\")\n",
                "\n",
                "for _, row in scenario_results.iterrows():\n",
                "    print(f\"\\nü§ñ Model: {row['model']}\")\n",
                "    print(f\"‚è±Ô∏è  Generation Time: {row['generation_time']:.2f}s\")\n",
                "    print(f\"‚≠ê Quality Score: {row['quality_score']}/10\")\n",
                "    print(f\"\\nüìú Generated Query:\")\n",
                "    print(\"-\" * 80)\n",
                "    print(row['query'])\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results to CSV\n",
                "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                "csv_filename = f'model_comparison_{timestamp}.csv'\n",
                "\n",
                "df_results.to_csv(csv_filename, index=False)\n",
                "print(f\"‚úÖ Results exported to: {csv_filename}\")\n",
                "\n",
                "# Save summary\n",
                "summary_filename = f'model_summary_{timestamp}.csv'\n",
                "summary.to_csv(summary_filename)\n",
                "print(f\"‚úÖ Summary exported to: {summary_filename}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusions & Recommendations\n",
                "\n",
                "Based on the comparison results:\n",
                "\n",
                "1. **Best Overall Model**: [To be determined from results]\n",
                "2. **Fastest Model**: [To be determined from results]\n",
                "3. **Most Accurate Model**: [To be determined from results]\n",
                "\n",
                "### Recommendations:\n",
                "\n",
                "- For **production use** where accuracy is critical: Use the model with highest quality score\n",
                "- For **real-time applications**: Use the fastest model with acceptable quality\n",
                "- For **complex queries**: Consider using multiple models and selecting the best output\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. Validate generated queries against real SIEM platforms\n",
                "2. Collect analyst feedback on query usefulness\n",
                "3. Fine-tune prompts for better quality\n",
                "4. Consider ensemble approaches combining multiple models"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}